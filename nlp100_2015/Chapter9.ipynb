{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章: ベクトル空間法 (I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current',\n",
       " '                                 Dload  Upload   Total   Spent    Left  Speed',\n",
       " '',\n",
       " '  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0',\n",
       " ' 40 21.0M   40 8605k    0     0  9555k      0  0:00:02 --:--:--  0:00:02 9551k',\n",
       " ' 92 21.0M   92 19.3M    0     0  10.1M      0  0:00:02  0:00:01  0:00:01 10.1M',\n",
       " '100 21.0M  100 21.0M    0     0  10.1M      0  0:00:02  0:00:02 --:--:-- 10.1M']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%system curl -O 'http://www.cl.ecei.tohoku.ac.jp/nlp100/data/enwiki-20150112-400-r100-10576.txt.bz2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80. コーパスの整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "\n",
    "sentences = []\n",
    "for line in bz2.BZ2File('enwiki-20150112-400-r100-10576.txt.bz2'):\n",
    "    sentences.append(' '.join([token.strip('.,!?;:()[]\\'\"').strip() for token in line.decode('utf8')[:-1].split(' ') if token.strip('.,!?;:()[]\\'\"').strip()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('corpus80.txt', 'w') as fd:\n",
    "    for sentence in sentences:\n",
    "        if sentence:\n",
    "            fd.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 81. 複合語からなる国名への対処"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_sovereign_states'\n",
    "dataframe = pd.io.html.read_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_names = {}\n",
    "for i in dataframe[0][4:-2][0]:\n",
    "    if i.startswith('ZZZ'):\n",
    "        continue\n",
    "    if '\\xa0– ' in i:\n",
    "        (country_name, long_country_name) = i.split('\\xa0– ')\n",
    "        country_names[country_name.split()[0]] = country_name\n",
    "        country_names[long_country_name.split()[0]] = long_country_name\n",
    "    else:\n",
    "        country_names[i.split()[0]] = i\n",
    "country_names['United'] = 'United States'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "with open('corpus80.txt') as fd:\n",
    "    for tokens in fd:\n",
    "        new_tokens = []\n",
    "        for (i, token) in enumerate(tokens.split()):\n",
    "            if token in country_names:\n",
    "                cn = country_names[token]\n",
    "                if cn == tokens[i:i+len(cn)]:\n",
    "                    cn = cn.replace(' ', '_')\n",
    "                    new_tokens.append(cn)\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        sentences.append(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 82. 文脈の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open('corpus82.txt', 'w') as fd:\n",
    "    for sentence in sentences:\n",
    "        for j in range(len(sentence)):\n",
    "            t = sentence[j]\n",
    "            d = random.randint(1, 5)\n",
    "            for k in range(max(j - d, 0), min(j + d + 1, len(sentence))):\n",
    "                if j != k:\n",
    "                    fd.write('%s\\t%s\\n' % (t, sentence[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83. 単語／文脈の頻度の計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "work_tc = []\n",
    "work_t = []\n",
    "work_c = []\n",
    "counter_tc = Counter()\n",
    "counter_t = Counter()\n",
    "counter_c = Counter()\n",
    "\n",
    "with open('corpus82.txt') as fd:\n",
    "    for (i, line) in enumerate(fd, start=1):\n",
    "\n",
    "        line = line.strip()\n",
    "        tokens = line.split('\\t')\n",
    "\n",
    "        work_tc.append(line)\n",
    "        work_t.append(tokens[0])\n",
    "        work_c.append(tokens[1])\n",
    "\n",
    "        if i % 1000000 == 0:\n",
    "            counter_tc.update(work_tc)\n",
    "            counter_t.update(work_t)\n",
    "            counter_c.update(work_c)\n",
    "            work_tc = []\n",
    "            work_t = []\n",
    "            work_c = []\n",
    "\n",
    "    counter_tc.update(work_tc)\n",
    "    counter_t.update(work_t)\n",
    "    counter_c.update(work_c)\n",
    "\n",
    "del work_tc, work_t, work_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67519664"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = sum(counter_tc.values())\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 84. 単語文脈行列の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from scipy import sparse\n",
    "\n",
    "dict_index_t = OrderedDict((key, i) for i, key in enumerate(counter_t.keys()))\n",
    "dict_index_c = OrderedDict((key, i) for i, key in enumerate(counter_c.keys()))\n",
    "\n",
    "size_t = len(dict_index_t)\n",
    "size_c = len(dict_index_c)\n",
    "matrix_x = sparse.lil_matrix((size_t, size_c))\n",
    "\n",
    "def calc_ppmi(N, f_tc, t, c):\n",
    "    if f_tc < 10:\n",
    "        return 0\n",
    "    return max(math.log((N * f_tc) / (counter_t[t] * counter_c[c])), 0)\n",
    "\n",
    "for (k, f_tc) in counter_tc.items():\n",
    "    if f_tc >= 10:\n",
    "        (t, c) = k.split('\\t')\n",
    "        ppmi = calc_ppmi(N, f_tc, t, c)\n",
    "        if ppmi > 0:\n",
    "            matrix_x[dict_index_t[t], dict_index_c[c]] = ppmi\n",
    "\n",
    "\n",
    "with open('dict_index_t.pkl', 'wb') as data_file:\n",
    "    pickle.dump(dict_index_t, data_file)\n",
    "del counter_tc, counter_t, counter_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<383413x383413 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 444172 stored elements in LInked List format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85. 主成分分析による次元圧縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.38399554e-13,   2.80623862e-13,  -6.93487765e-13, ...,\n",
       "          1.48299369e-12,  -4.60395097e-12,   2.09304182e-12],\n",
       "       [  1.07918219e+01,  -9.79396744e+00,   1.39427786e+01, ...,\n",
       "         -2.06871316e-01,   1.52703964e-01,  -7.67869430e-02],\n",
       "       [  1.33383104e+01,  -1.10493041e+01,   2.16714974e+01, ...,\n",
       "          4.74495582e-02,   7.17963788e-02,  -4.22875343e-02],\n",
       "       ..., \n",
       "       [  0.00000000e+00,  -0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,  -0.00000000e+00,  -0.00000000e+00],\n",
       "       [  0.00000000e+00,  -0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,  -0.00000000e+00,  -0.00000000e+00],\n",
       "       [  0.00000000e+00,  -0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,  -0.00000000e+00,  -0.00000000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.decomposition\n",
    "from scipy import io\n",
    "\n",
    "clf = sklearn.decomposition.TruncatedSVD(300)\n",
    "matrix_x300 = clf.fit_transform(matrix_x)\n",
    "io.savemat('matrix_x300', {'matrix_x300': matrix_x300})\n",
    "matrix_x300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 86. 単語ベクトルの表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17591139 -0.04139807 -0.02054507  0.47802428  0.28762109 -0.14161546\n",
      " -0.25939585  0.12568658  0.04667047 -0.01762385  0.00440574  0.23515148\n",
      "  0.06984147  0.36296715  0.1945541   0.38576479  0.04448114 -0.50779538\n",
      " -0.00533639  0.07326178  0.0331656   0.12731005  0.03327558  0.0320777\n",
      " -0.00250625  0.04143549 -0.04168525  0.27192375 -0.28981559  0.65676299\n",
      "  0.08531146  0.06546287  0.04940578 -0.02037115  0.05607366 -0.0011302\n",
      "  0.52342221  0.22832209  0.52550436  0.04312807  0.17397772 -0.33513058\n",
      "  0.20112464 -0.03150447  0.19837785  0.4667724   0.03302099 -0.1628859\n",
      "  0.0663168  -0.13928939  0.01789095 -0.17317036 -0.08631318 -0.01488529\n",
      "  0.06342893  0.12190213 -0.01892328  0.00207561 -0.05805681  0.05262019\n",
      " -0.05886119 -0.14166798  0.11959298  0.39347617  0.40175756  0.22269925\n",
      "  0.35954049 -0.10378227 -0.20258596  0.12939298 -0.14465286  0.2667883\n",
      " -0.08647199 -0.27886252 -0.13528828  0.19597011 -0.02525106  0.10937388\n",
      " -0.11690274  0.09950905 -0.61639285 -0.31738436  0.43136758  0.01288661\n",
      "  0.30763346 -0.33620962  0.08271438 -0.04292478  0.20180193  0.29744806\n",
      " -0.43527655  0.12481453 -0.03787312  0.14820233  0.17194258  0.33376976\n",
      " -0.1127305   0.01173611  0.19430726 -0.05296943  0.08845936 -0.1983313\n",
      "  0.07223773 -0.02478306  0.5595543  -0.04084596 -0.04008983  0.4921925\n",
      " -0.31816977 -0.39134874  0.39809054 -0.04629494  0.07829501 -0.15061438\n",
      " -0.11106595  0.27128214  0.17135426 -0.71182305  0.20510195  0.54809751\n",
      " -0.35736136 -0.39125418  0.00388371  0.27284237 -0.02097862 -0.18133961\n",
      "  0.05931005 -0.25869398 -0.27792137 -0.09201403  0.11254891 -0.07392666\n",
      " -0.01607525  0.08407504 -0.3212321  -0.22423043  0.24371707 -0.87422418\n",
      "  0.11512298  0.10934234  0.247425    0.09762334  0.44787854  0.21493863\n",
      "  0.01116872 -0.51086066 -0.17349126  0.26048344  0.10587766 -0.1261747\n",
      "  0.07551329  0.30511371  0.2210401  -0.2732935  -0.53998341 -0.24334588\n",
      " -0.12151964  0.64876366 -0.07528252 -0.28943471  0.04411346  0.28241084\n",
      "  0.37490233 -0.17857198  0.04607     0.52421702  0.2622075  -0.1962366\n",
      "  0.15058283  0.07621635 -0.0799252   0.08558031 -0.25277528 -0.00588408\n",
      "  0.4224224  -0.54013432  0.14076467  0.31261855 -0.10360132  0.13119789\n",
      "  0.01376859  0.34619965  0.22068727 -0.33774759  0.37468247  0.17494743\n",
      " -0.07777483  0.10202714 -0.1371646  -0.33130594  0.25608773  0.3240164\n",
      " -0.0708713  -0.15985348  0.13202939  0.00140505  0.08206496 -0.45151699\n",
      " -0.29851188  0.06195673  0.38084471  0.18605514 -0.25389035 -0.01382513\n",
      "  0.01158944 -0.04931353 -0.35348499 -0.08697639 -0.29034693 -0.08042969\n",
      "  0.31689835 -0.14297183  0.44292136 -0.05223191  0.07325866  0.04206026\n",
      "  0.06538689  0.04578291 -0.45332707  0.01346925 -0.07738494  0.18987229\n",
      "  0.04419079 -0.02204672 -0.08189548  0.08332137 -0.31587932  0.25558047\n",
      "  0.15973255 -0.35539699 -0.08450208  0.00620161 -0.03910648 -0.22464448\n",
      " -0.27696488  0.07187858  0.00616283  0.22586426 -0.4653325   0.1574137\n",
      " -0.01674747 -0.12933798 -0.31119614  0.10333572  0.12621921 -0.32787449\n",
      " -0.00607735 -0.28461356  0.04955393  0.31686192  0.23636612  0.36436652\n",
      "  0.16533956  0.18984974 -0.06670212  0.01458994  0.05898793  0.09833234\n",
      " -0.02407226 -0.04486626 -0.41870337  0.12609602 -0.47145994 -0.33293545\n",
      " -0.31744129 -0.06591356  0.22479238 -0.11168505  0.30760221  0.35626827\n",
      "  0.39169713 -0.13398567 -0.19228239 -0.02422618 -0.39516762 -0.05854141\n",
      " -0.38830247  0.20896548  0.03958175  0.12225299 -0.00691976 -0.01208547\n",
      "  0.42756604 -0.11306722  0.22105031 -0.23185821  0.16419281  0.34863713\n",
      " -0.35228981  0.51960341 -0.26127081  0.1011223  -0.01259696  0.00148486\n",
      " -0.33171475 -0.03254656  0.08345168 -0.11081284 -0.1516299   0.1928701 ]\n"
     ]
    }
   ],
   "source": [
    "print(matrix_x300[dict_index_t['United_States']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87. 単語の類似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30363386324630748"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    norm = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "    if norm != 0:\n",
    "        return np.dot(a, b) / norm\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "cos_sim(matrix_x300[dict_index_t['United_States']], matrix_x300[dict_index_t['U.S']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 88. 類似度の高い単語10件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scotland\t0.659968292937\n",
      "Wales\t0.600579664717\n",
      "Ireland\t0.498700869599\n",
      "Patriots\t0.494463110596\n",
      "Africa\t0.484252012028\n",
      "Cheshire\t0.470073555068\n",
      "Zealand\t0.46144041119\n",
      "America\t0.432902417873\n",
      "Europe\t0.428471506011\n",
      "coast\t0.418348492945\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    norm = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "    if norm != 0:\n",
    "        return np.dot(a, b) / norm\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "vec_england = matrix_x300[dict_index_t['England']]\n",
    "distances = [cos_sim(vec_england, matrix_x300[i]) for i in range(0, len(dict_index_t))]\n",
    "\n",
    "index_sorted = np.argsort(distances)\n",
    "keys = list(dict_index_t.keys())\n",
    "for index in index_sorted[-2:-12:-1]:\n",
    "    print('%s\\t%s' % (keys[index], distances[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 89. 加法構成性によるアナロジー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freiburg\t0.57023386464\n",
      "Tezpur\t0.570186819117\n",
      "Leiden\t0.570126297876\n",
      "McGill\t0.569098322224\n",
      "Nehru\t0.56906406981\n",
      "EMUNI\t0.568647371521\n",
      "Loyola\t0.566776913062\n",
      "Peking\t0.565936583607\n",
      "Yeshiva\t0.565890244762\n",
      "Emory\t0.56575074229\n"
     ]
    }
   ],
   "source": [
    "vec = matrix_x300[dict_index_t['Spain']] - matrix_x300[dict_index_t['Madrid']] + matrix_x300[dict_index_t['Athens']]\n",
    "distances = [cos_sim(vec, matrix_x300[i]) for i in range(len(dict_index_t))]\n",
    "\n",
    "index_sorted = np.argsort(distances)\n",
    "keys = list(dict_index_t.keys())\n",
    "for index in index_sorted[:-11:-1]:\n",
    "    print('%s\\t%s' % (keys[index], distances[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
